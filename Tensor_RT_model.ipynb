{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb50Y-41kHQ9",
        "outputId": "39cf4a17-24ce-45bd-ce6d-4cf61c948fe8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-sEPq-3xlSZ",
        "outputId": "c2d31209-f893-4294-df2f-4890a1f9af60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Thu Dec 26 10:43:58 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,630 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,566 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,614 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,830 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Fetched 26.9 MB in 4s (7,192 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvinfer-headers-dev libnvinfer10\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-dev libnvinfer-headers-dev libnvinfer-plugin8 libnvinfer10\n",
            "  libnvinfer8\n",
            "0 upgraded, 5 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 2,989 MB of archives.\n",
            "After this operation, 7,707 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-headers-dev 10.7.0.23-1+cuda12.6 [106 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer10 10.7.0.23-1+cuda12.6 [1,240 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-dev 10.7.0.23-1+cuda12.6 [1,246 MB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer8 8.6.1.6-1+cuda12.0 [492 MB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvinfer-plugin8 8.6.1.6-1+cuda12.0 [11.7 MB]\n",
            "Fetched 2,989 MB in 1min 59s (25.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer-headers-dev.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libnvinfer-headers-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer10.\n",
            "Preparing to unpack .../libnvinfer10_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../libnvinfer-dev_10.7.0.23-1+cuda12.6_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Selecting previously unselected package libnvinfer8.\n",
            "Preparing to unpack .../libnvinfer8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Selecting previously unselected package libnvinfer-plugin8.\n",
            "Preparing to unpack .../libnvinfer-plugin8_8.6.1.6-1+cuda12.0_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-headers-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer10 (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer-dev (10.7.0.23-1+cuda12.6) ...\n",
            "Setting up libnvinfer8 (8.6.1.6-1+cuda12.0) ...\n",
            "Setting up libnvinfer-plugin8 (8.6.1.6-1+cuda12.0) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Cloning into 'tensorflow'...\n",
            "remote: Enumerating objects: 1939887, done.\u001b[K\n",
            "remote: Counting objects: 100% (1057/1057), done.\u001b[K\n",
            "remote: Compressing objects: 100% (452/452), done.\u001b[K\n",
            "remote: Total 1939887 (delta 869), reused 606 (delta 605), pack-reused 1938830 (from 3)\u001b[K\n",
            "Receiving objects: 100% (1939887/1939887), 1.06 GiB | 25.39 MiB/s, done.\n",
            "Resolving deltas: 100% (1594200/1594200), done.\n",
            "Updating files: 100% (34443/34443), done.\n",
            "/content/tensorflow\n",
            "Updating files: 100% (27911/27911), done.\n",
            "Branch 'r2.10' set up to track remote branch 'r2.10' from 'origin'.\n",
            "Switched to a new branch 'r2.10'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)'\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `('\n",
            "/bin/bash: -c: line 1: `bazel install ... (Install the built TensorFlow package)'\n"
          ]
        }
      ],
      "source": [
        "# Ensure CUDA and cuDNN are installed\n",
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "# Install the required dependencies for building TensorFlow with TensorRT support\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y libnvinfer8 libnvinfer-dev libnvinfer-plugin8\n",
        "# (Install other necessary packages as mentioned in TensorFlow documentation)\n",
        "# Clone the TensorFlow repository and checkout the desired branch\n",
        "!git clone https://github.com/tensorflow/tensorflow.git\n",
        "%cd tensorflow\n",
        "!git checkout r2.10 # Check the TensorFlow-TensorRT compatibility matrix for the correct branch.\n",
        "# Configure TensorFlow build with TensorRT enabled\n",
        "# ./configure\n",
        "# (During configuration, enable TensorRT support when prompted)\n",
        "# If you are using a virtual environment, activate it before building TensorFlow.\n",
        "# Build and install TensorFlow\n",
        "!bazel build --config=cuda --config=monolithic ... (Specify the build target with TensorRT support)\n",
        "!bazel install ... (Install the built TensorFlow package)\n",
        "# After successful installation, restart the runtime to ensure the new TensorFlow installation is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dvcSHgfz8m9",
        "outputId": "e7b3177c-9293-47e5-b369-ad4f6245dc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 26 10:54:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "MwutDdg51rgy",
        "outputId": "01f8442c-2fd0-463e-f0c8-872792ae3c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.26.4)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (24.3.25)\n",
            "Collecting protobuf~=3.20 (from tf2onnx)\n",
            "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2024.12.14)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, tf2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 protobuf-3.20.3 tf2onnx-1.16.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7d69d1e8245c4614a6942baafa30af55"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tf2onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNXWqRfj3yct",
        "outputId": "5ec32623-6a4e-4a4e-81dd-11f200b05f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n"
          ]
        }
      ],
      "source": [
        "%cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YsAveA1Z0b14"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JVkXuSvF4HIX"
      },
      "outputs": [],
      "source": [
        "def convert_keras_to_tensorrt(keras_model_path, trt_model_dir):\n",
        "    \"\"\"\n",
        "    Converts a Keras model to a TensorRT-optimized SavedModel.\n",
        "\n",
        "    Args:\n",
        "        keras_model_path (str): Path to the .keras model file.\n",
        "        trt_model_dir (str): Directory where the TensorRT-optimized model will be saved.\n",
        "    \"\"\"\n",
        "    print(\"Loading Keras model...\")\n",
        "    # Load the Keras model\n",
        "    keras_model = tf.keras.models.load_model(keras_model_path)\n",
        "\n",
        "    # Create a temporary directory to save the SavedModel\n",
        "    temp_saved_model_dir = \"temp_saved_model\"\n",
        "    os.makedirs(temp_saved_model_dir, exist_ok=True)\n",
        "\n",
        "    # Save the Keras model as a SavedModel\n",
        "    print(\"Saving Keras model as SavedModel...\")\n",
        "    tf.saved_model.save(keras_model, temp_saved_model_dir)\n",
        "\n",
        "    print(\"Converting Keras model to TensorRT...\")\n",
        "    # Initialize the TensorRT converter\n",
        "    params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
        "        precision_mode=trt.TrtPrecisionMode.FP16,  # Use FP16 for faster inference (if supported)\n",
        "        max_workspace_size_bytes=1 << 30          # 1GB workspace size\n",
        "    )\n",
        "    # Pass the temporary SavedModel directory to the converter\n",
        "    converter = trt.TrtGraphConverterV2(input_saved_model_dir=temp_saved_model_dir, conversion_params=params)\n",
        "\n",
        "    # Convert the Keras model\n",
        "    converter.convert()\n",
        "\n",
        "    # Save the TensorRT-optimized model\n",
        "    print(f\"Saving TensorRT-optimized model to {trt_model_dir}...\")\n",
        "    converter.save(trt_model_dir)\n",
        "    print(f\"TensorRT-optimized model saved at {trt_model_dir}\")\n",
        "\n",
        "    # Optionally remove the temporary SavedModel directory\n",
        "    # import shutil\n",
        "    # shutil.rmtree(temp_saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "O6diCWd74TF3"
      },
      "outputs": [],
      "source": [
        "def load_tensorrt_model_and_infer(trt_model_dir, input_data):\n",
        "    \"\"\"\n",
        "    Loads a TensorRT-optimized model and performs inference.\n",
        "\n",
        "    Args:\n",
        "        trt_model_dir (str): Directory of the TensorRT-optimized model.\n",
        "        input_data (numpy.ndarray): Input data for inference.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Model predictions.\n",
        "    \"\"\"\n",
        "    print(\"Loading TensorRT-optimized model...\")\n",
        "    trt_model = tf.saved_model.load(trt_model_dir)\n",
        "    infer = trt_model.signatures[\"serving_default\"]\n",
        "\n",
        "    print(\"Running inference...\")\n",
        "    # Perform inference\n",
        "    predictions = infer(tf.convert_to_tensor(input_data))\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vCv6ykd64Wa4"
      },
      "outputs": [],
      "source": [
        "keras_model_path = \"/content/drive/MyDrive/trained models/cifar10_augmented_model.keras\"         # Path to your .keras model\n",
        "trt_model_dir = \"/content/drive/MyDrive/trained models/model_trt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BH8lLzz4eeZ",
        "outputId": "52e5f35f-387d-4c89-e4fb-489545fe8758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Keras model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 34 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Keras model as SavedModel...\n",
            "Converting Keras model to TensorRT...\n",
            "Saving TensorRT-optimized model to /content/drive/MyDrive/trained models/model_trt...\n",
            "TensorRT-optimized model saved at /content/drive/MyDrive/trained models/model_trt\n"
          ]
        }
      ],
      "source": [
        "convert_keras_to_tensorrt(keras_model_path, trt_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "CwFDsNn7nXLp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Original model\n",
        "original_model = load_model('/content/drive/MyDrive/trained models/cifar10_augmented_model.keras')\n",
        "\n",
        "# Save the original model as a SavedModel\n",
        "tf.saved_model.save(original_model, '/content/original_saved_model')\n",
        "\n",
        "# Load the original model as a SavedModel\n",
        "original_saved_model = tf.saved_model.load('/content/original_saved_model')\n",
        "\n",
        "# Now you can access the signatures\n",
        "original_serving_fn = original_saved_model.signatures['serving_default']\n",
        "\n",
        "# TensorRT-optimized model (no changes needed)\n",
        "optimized_model = tf.saved_model.load('/content/drive/MyDrive/trained models/model_trt')\n",
        "optimized_serving_fn = optimized_model.signatures['serving_default']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oXZ4SrXniJi",
        "outputId": "2c721a61-b496-431a-924c-41b2c88f8869"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 18 variables whereas the saved optimizer has 34 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample batch of data for benchmarking\n",
        "input_shape = (32, 32, 3)\n",
        "batch_size = 32\n",
        "dummy_input = np.random.random((batch_size, *input_shape)).astype(np.float32)\n",
        "original_input = tf.convert_to_tensor(dummy_input)\n",
        "optimized_input = tf.convert_to_tensor(dummy_input)"
      ],
      "metadata": {
        "id": "lUIX3dstnnId"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Import the 'time' module\n",
        "\n",
        "def measure_latency(model_fn, input_key, input_tensor, warmup=10, iterations=100):\n",
        "    \"\"\"\n",
        "    Measures the average latency of a TensorFlow model.\n",
        "\n",
        "    Args:\n",
        "        model_fn: The model function to be measured.\n",
        "        input_key (str): The key used to pass the input tensor to the model function.\n",
        "                           Should be 'inputs' for SavedModel signatures.\n",
        "        input_tensor: The input tensor to be used for inference.\n",
        "        warmup (int): The number of warm-up runs to perform before measuring latency.\n",
        "        iterations (int): The number of iterations to run for latency measurement.\n",
        "\n",
        "    Returns:\n",
        "        float: The average latency in milliseconds.\n",
        "    \"\"\"\n",
        "\n",
        "    # Warm-up runs\n",
        "    for _ in range(warmup):\n",
        "        _ = model_fn(**{input_key: input_tensor})  # Pass input using the correct key\n",
        "\n",
        "    # Measure latency\n",
        "    start_time = time.time()\n",
        "    for _ in range(iterations):\n",
        "        _ = model_fn(**{input_key: input_tensor})  # Pass input using the correct key\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Calculate average latency\n",
        "    avg_latency = (end_time - start_time) / iterations\n",
        "    return avg_latency * 1000  # Convert to milliseconds"
      ],
      "metadata": {
        "id": "w6UgacxKopmK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_latency = measure_latency(\n",
        "    original_serving_fn, \"inputs\", original_input # Changed input_key to 'inputs'\n",
        ")"
      ],
      "metadata": {
        "id": "zGZFEao0osix"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_latency = measure_latency(\n",
        "    optimized_serving_fn, \"inputs\", optimized_input\n",
        ")"
      ],
      "metadata": {
        "id": "wejWiML8ov95"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original Model Latency: {original_latency:.2f} ms\")\n",
        "print(f\"Optimized Model Latency: {optimized_latency:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43DtSX0Mox6g",
        "outputId": "3bc6cdc1-572d-48e2-f7b0-6d11bf9c869e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Latency: 2.56 ms\n",
            "Optimized Model Latency: 0.56 ms\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}